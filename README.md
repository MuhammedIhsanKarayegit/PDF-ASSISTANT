# ğŸ“„ PDF AsistanÄ± (Local RAG Desktop App)


Bu proje, PDF dokÃ¼manlarÄ±nÄ±zla **yerel olarak (offline)** sohbet etmenizi saÄŸlayan, kullanÄ±cÄ± dostu bir masaÃ¼stÃ¼ uygulamasÄ±dÄ±r. Verileriniz hiÃ§bir bulut sunucusuna gitmez; **Ollama (Llama 3)**, **LangChain** ve **ChromaDB** kullanÄ±larak tamamen kendi bilgisayarÄ±nÄ±zda iÅŸlenir.

## ğŸŒŸ Ã–zellikler

* **ğŸš€ Tamamen Yerel Ã‡alÄ±ÅŸma:** Ä°nternet baÄŸlantÄ±sÄ± gerektirmez (Model indirildikten sonra), verileriniz bilgisayarÄ±nÄ±zda kalÄ±r.
* **ğŸ§  AkÄ±llÄ± HafÄ±za:** Asistan, sohbet geÃ§miÅŸini hatÄ±rlar ve baÄŸlama uygun cevaplar verir.
* **âš¡ PerformanslÄ± ArayÃ¼z (Threading):** PDF yÃ¼kleme ve cevap oluÅŸturma iÅŸlemleri arka planda (Thread) yapÄ±lÄ±r, arayÃ¼z asla donmaz.
* **ğŸ¨ Modern TasarÄ±m:** GÃ¶z yormayan, ÅŸÄ±k **Dark Mode** (KaranlÄ±k Tema) ve WhatsApp tarzÄ± mesaj balonlarÄ±.
* **ğŸ“‚ VektÃ¶r VeritabanÄ±:** DokÃ¼manlar parÃ§alanÄ±r ve ChromaDB ile vektÃ¶rleÅŸtirilerek hÄ±zlÄ± arama yapÄ±lÄ±r.
* **ğŸ”„ Oturum YÃ¶netimi:** "Yeni Sohbet" Ã¶zelliÄŸi ile hafÄ±za ve veritabanÄ± tek tÄ±kla temizlenir.

## ğŸ› ï¸ KullanÄ±lan Teknolojiler

* **Backend:** Python
* **AI Framework:** LangChain (ConversationalRetrievalChain)
* **LLM & Embeddings:** Ollama (Llama 3 Modeli)
* **VeritabanÄ±:** ChromaDB (VektÃ¶r VeritabanÄ±)
* **ArayÃ¼z (GUI):** PySide6 (Qt)

## âš™ï¸ Kurulum

Proje dosyalarÄ±nÄ± bilgisayarÄ±nÄ±za indirdikten sonra aÅŸaÄŸÄ±daki adÄ±mlarÄ± izleyin.

### 1. Gereksinimler

BilgisayarÄ±nÄ±zda [Ollama](https://ollama.com/) kurulu olmalÄ± ve Llama 3 modeli Ã§ekilmelidir:

```bash
ollama pull llama3

```

### 2. Sanal Ortam ve KÃ¼tÃ¼phaneler

Tercih ettiÄŸiniz terminalde proje dizinine gelin:

```bash
# Gerekli kÃ¼tÃ¼phaneleri yÃ¼kleyin
pip install langchain langchain-community langchain-ollama langchain-chroma PySide6

```

*(Not: EÄŸer `langchain` sÃ¼rÃ¼m hatasÄ± alÄ±rsanÄ±z 0.3.x sÃ¼rÃ¼mÃ¼nÃ¼ kullandÄ±ÄŸÄ±nÄ±zdan emin olun.)*

### 3. Ã‡alÄ±ÅŸtÄ±rma

UygulamayÄ± baÅŸlatmak iÃ§in:

```bash
python src/main.py

```


# ğŸ“„ PDF Assistant (English)

This project is a user-friendly desktop application that allows you to chat with your PDF documents **locally (offline)**. Your data never leaves your computer; everything is processed locally using **Ollama (Llama 3)**, **LangChain**, and **ChromaDB**.

## ğŸŒŸ Features

* **ğŸš€ Fully Local:** No internet connection required (after model download), ensuring data privacy.
* **ğŸ§  Conversational Memory:** The assistant remembers the chat history and provides context-aware answers.
* **âš¡ Responsive UI (Threading):** Heavy tasks like PDF ingestion and generation run on background threads, preventing UI freeze.
* **ğŸ¨ Modern Design:** Sleek **Dark Mode** interface with modern, chat-app style message bubbles.
* **ğŸ“‚ Vector Database:** Documents are split and vectorized using ChromaDB for fast retrieval.
* **ğŸ”„ Session Management:** "New Chat" button instantly clears memory and database for a fresh start.

## ğŸ› ï¸ Tech Stack

* **Backend:** Python
* **AI Framework:** LangChain (ConversationalRetrievalChain)
* **LLM & Embeddings:** Ollama (Llama 3 Model)
* **Database:** ChromaDB (Vector Store)
* **Interface (GUI):** PySide6 (Qt)

## âš™ï¸ Installation

Follow these steps after cloning/downloading the repository.

### 1. Prerequisites

Ensure [Ollama](https://ollama.com/) is installed and the Llama 3 model is pulled:

```bash
ollama pull llama3

```

### 2. Environment & Dependencies

Navigate to the project directory in your terminal:

```bash
# Install required packages
pip install langchain langchain-community langchain-ollama langchain-chroma PySide6

```

*(Note: Ensure you are using LangChain version 0.3.x or compatible.)*

### 3. Usage

Run the application:

```bash
python src/main.py

```

## ğŸ“œ License

This project is open-source and available under the [MIT License](https://www.google.com/search?q=LICENSE).